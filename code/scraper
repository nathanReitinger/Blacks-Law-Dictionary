
import sys
import pandas as pd
import time
import requests
from bs4 import BeautifulSoup 

# NOTE #
# - sovereignconnection's index page is down right now


##########################################################################################
#  import each term and path 
##########################################################################################

file = pd.DataFrame(columns=('term', 'path', 'definition'))
startRow = 0
 
for i in range(15):                                            # this many pages at time of scrape
     
    count = i       
                 
    path = 'http://dictionary.sovereignconnection.com/?pgno=' 
    path = path + str(count) + '#azindex-1'
     
     
    page = requests.get(path) 
    if 200 != page.status_code: 
        sys.exit("system exiting...error code")
    soup = BeautifulSoup(page.text,'lxml')
     
     
    entry = soup.find('div', {'id': 'azindex-1'})
    for a in entry.find_all('a', href=True):
        term = a.get_text()                                     # for now, we just get the terms (on an index-like page)
        print(term)
        termPath = a['href']                                    # grap path for lookup in next scrape
        print(termPath)
        file.loc[startRow,"term"] = term
        file.loc[startRow,"path"] = termPath
        startRow += 1
 
# print(file.head())
# print(file.tail())
 
file.to_csv('blacks8.csv')



##########################################################################################
#  fill in each term's definition, crawling cell with path 
##########################################################################################
file = pd.read_csv('blacks8.csv')
# print(len(file))
# print(file.head())
  
startRow = 0
       
for index, row in file.iterrows():
    path = row[1]
    term = row[0]
          
    page = requests.get(path)
    if 200 != page.status_code: 
        sys.exit("system exiting...error code", page.status_code)
              
    soup = BeautifulSoup(page.content, 'html.parser')
#         entry = soup.find('div', {'class': 'content'}).prettify(formatter=None)
    
    rows = soup.find_all('p')
       
    definition = ""
       
    for i in range(len(rows)):
        entryLines = (rows[i].get_text(strip=True))
        definition = definition + entryLines
              
#     print(definition)
  
    file.loc[startRow,"definition"] = definition
    startRow += 1
    time.sleep(2)  
    file.to_csv('blacks8_with_definitions.csv', encoding='utf-8') 


   
    
